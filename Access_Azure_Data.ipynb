{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b332b40a-dd70-4e2c-9f56-785bb96f919b",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "# The following cells contains different methods to access data from azure storage in databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b40f7f-75fe-49ce-9bb2-f9c5298f0a0c",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Access data using Storage Access Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd1a3ec5-e016-4101-a598-5649138939a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"\"\n",
    "container_name = \"\"\n",
    "access_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "044c33e1-6444-4079-8e63-84e325a6657f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\", access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe0af738-6c99-4381-982a-c7f261287901",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"adfss://{container_name}@{storage_account}.dfs.core.windows.net\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d925d73-d5b7-4c7d-9012-64629ad79d0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Access data using Shared Access Signature (SAS) tokens\n",
    "**Note:**\n",
    "1. provides fine grained access to storage\n",
    "2. restrict access to specific resource types\n",
    "3. allow specific permission like read-only\n",
    "4. restrict access to specific time period\n",
    "5. limit access to specific IP address\n",
    "6. recommended access pattern for external clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6e7a52-899f-4da5-8158-138951ded78f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"\"\n",
    "container_name = \"\"\n",
    "sas_token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb2312c0-5ac1-4125-a4ba-64572045c5f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\"\n",
    "              )\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a6cc770-3453-423e-b544-aa3b3fdafb22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"adfss://{container_name}@{storage_account}.dfs.core.windows.net\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "227a41ba-9de0-4cf2-afaa-458a961719eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Access data using Service Principal\n",
    "**Note:**\n",
    "1. quite similar to user accounts\n",
    "2. these are registered in AAD and assigned permissions to access resources using ROle based access control (RBAC)\n",
    "3. recommended to use in databricks jobs and CI/CD pipelines\n",
    "4. provides better security and monitoring as they can be audited\n",
    "\n",
    "**Steps to follow:**\n",
    "1. register azure AD application / Service principal\n",
    "2. generate a secret/password for the principal\n",
    "3. set spark config with app/client ID, directory/tenant ID and secret\n",
    "4. assign role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b04aa1a9-4a23-48cb-b58b-093cea355c7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"\"\n",
    "container_name = \"\"\n",
    "client_id = \"\"\n",
    "tenant_id = \"\"\n",
    "client_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8332caf1-a245-4e29-9275-c381bd7d518a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"\n",
    "              )\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\",\n",
    "               f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e801e7e4-fb4e-4756-8a88-d2d9a6379f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(f\"adfss://{container_name}@{storage_account}.dfs.core.windows.net\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ad30573-738d-4556-8b9c-02f260c55581",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Access data using AAD Credential Passthrough\n",
    "**Note:**\n",
    "1. used to restrict users based on what they can see via AAD account\n",
    "2. Databricks will pass the users, AAD credentials to the storage accountto authenticate. If the specific user has the required role assigned in RBAC for the storage account, they'll be able to access a storage account otherwise, they won't be able to access the storage account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "272c5488-f5d7-450c-a4a9-ab914049de21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Fetch secrets from Azure key vaults using Databricks secret scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fce0bca-55e1-4ac7-bafa-b2b798369679",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scope_name = \"\"\n",
    "client_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7fcfbec-560d-42d8-af1e-abcc583f81fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59795600-8df2-4a67-9112-7850e29b2b06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.listScopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "463b548f-5595-4db6-8ba3-363e6b399cf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.list(scope=scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9947fcd-0bf9-4db5-a91c-f419f576e6a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.get(scope=scope_name,key=client_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b18fc3b-7fc4-4600-8a04-80bb20e3b90c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Mount azure blob storage using service principal\n",
    "**Steps to follow:**\n",
    "1. Get client id, tenant id and client value form key vault\n",
    "2. Set spark config with App/CLient ID, Directory/Tenant ID and Secret\n",
    "3. Call the file system utility mount, to mount the storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "825606ab-108f-4130-ad30-e4e1022763a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mount_adfs(storage_account, container_name, scope_name, client_key, tenant_key, secret_key):\n",
    "    # Get secrets from Key Vault\n",
    "    client_id = dbutils.secrets.get(scope = scope_name, key = client_key)\n",
    "    tenant_id = dbutils.secrets.get(scope = scope_name, key = tenant_key)\n",
    "    client_secret = dbutils.secrets.get(scope = scope_name, key = secret_key)\n",
    "    \n",
    "    # Set sprark configurations\n",
    "    configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "              \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "              \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "              \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "              \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"}\n",
    "    \n",
    "    if any(mount.mountPoint == f\"/mnt/{storage_account}/{container_name}\" for mount in dbutils.fs.mounts()):\n",
    "        dbutils.fs.unmount(f\"/mnt/{storage_account}/{container_name}\")\n",
    "        \n",
    "    # Mount the storage account container\n",
    "    dbutils.fs.mount(\n",
    "      source = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net\",\n",
    "      mount_point = f\"/mnt/{storage_account}/{container_name}\",\n",
    "      extra_configs = configs)\n",
    "    \n",
    "    display(dbutils.fs.mounts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe82d481-f80b-4044-9693-2637804a64ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.mounts())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Access_Azure_Data",
   "notebookOrigID": 3790368410462224,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "airlines",
   "language": "python",
   "name": "airlines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
