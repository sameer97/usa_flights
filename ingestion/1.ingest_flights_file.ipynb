{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f12401-689e-4491-8065-576e8cf80134",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ingest flight_data.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370fa607-fd6d-47a0-9033-596b551dbdd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"data_source\", \"\")\n",
    "data_source = dbutils.widgets.get(\"data_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"file_date\", \"2000-01-01\")\n",
    "file_date = dbutils.widgets.get(\"file_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e1f12b4-741d-4374-a5f9-57afcd25ac21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../includes/configurations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22eed0ca-13fd-4b3d-8602-0cf36bf52e61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db38c4cf-2749-4ce5-b8f5-65709783f503",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    BooleanType\n",
    ")\n",
    "from pyspark.sql.functions import current_timestamp, concat_ws, lit, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b80a990d-2b83-4368-b852-8a092ea26aa3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 1 - Read the CSV file using spark dataframe reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66cbed18-dda0-430c-8c3f-fde2b036b7dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_schema = StructType(fields=[\n",
    "    StructField('ActualElapsedTime', IntegerType(), True),\n",
    "    StructField('AirTime', IntegerType(), True),\n",
    "    StructField('ArrDelay', IntegerType(), True),\n",
    "    StructField('ArrTime', IntegerType(), True),\n",
    "    StructField('CRSArrTime', IntegerType(), False),\n",
    "    StructField('CRSDepTime', IntegerType(), False),\n",
    "    StructField('CRSElapsedTime', IntegerType(), False),\n",
    "    StructField('CancellationCode', StringType(), True),\n",
    "    StructField('Cancelled', IntegerType(), True),\n",
    "    StructField('CarrierDelay', IntegerType(), True),\n",
    "    StructField('DayOfWeek', IntegerType(), False),\n",
    "    StructField('DayofMonth', IntegerType(), False),\n",
    "    StructField('DepDelay', IntegerType(), True),\n",
    "    StructField('DepTime', IntegerType(), True),\n",
    "    StructField('Dest', StringType(), False),\n",
    "    StructField('Distance', IntegerType(), True),\n",
    "    StructField('Diverted', IntegerType(), False),\n",
    "    StructField('FlightNum', IntegerType(), False),\n",
    "    StructField('LateAircraftDelay', IntegerType(), True),\n",
    "    StructField('Month', IntegerType(), False),\n",
    "    StructField('NASDelay', IntegerType(), True),\n",
    "    StructField('Origin', StringType(), False),\n",
    "    StructField('SecurityDelay', IntegerType(), True),\n",
    "    StructField('TailNum', StringType(), False),\n",
    "    StructField('TaxiIn', IntegerType(), True),\n",
    "    StructField('TaxiOut', IntegerType(), True),\n",
    "    StructField('UniqueCarrier', StringType(), False),\n",
    "    StructField('WeatherDelay', IntegerType(), True),\n",
    "    StructField('Year', IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c321f3d-96cc-4163-82b3-afbf8dfa6ed8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_df = (spark.read.\n",
    "              option(\"header\", True).\n",
    "              schema(flights_schema).\n",
    "              csv(f\"{raw_folder_path}/{file_date}/flight_data.csv\")\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f21cdd3-3f84-4f0e-aabf-a90336aaa68a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Note:**\n",
    "\n",
    "- With *inferSchema as True*, what's happening is a Spark going through the data and reading all of the data, identify what the schema should be and then apply that schema to the DataFrame, which is not efficient because it has to read through the data. And in a production environment, it's going to be an awful lot of data and it could just slow down the reads\n",
    "\n",
    "- And also if you get data which doesn't confirm to what you're expecting, you want your process to fail and tell you that there is something wrong rather than just inferring the schema and carrying on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83055100-ae26-430d-bff5-eb22fd842533",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(flights_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42cf7a9-74a6-437c-98d8-fb1fd767e134",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394dd577-35d6-4793-b9a0-b20d657e37fd",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "source": [
    "##### Step 2 - Drop the unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c69e7de-cae1-4df7-866e-c7fd14305433",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_dropped_df = flights_df.drop(col(\"TailNum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7d2f93-a97d-4f8e-aecc-e73d3c74d89e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 3 - Rename the columns as requried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb14e821-02f9-480a-8f40-7081d88b37ab",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from src import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d1f439-15e3-462b-acb6-7dde5bb9a304",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_cfg = ut.load_yml(\"ingest_conf.yml\")\n",
    "column_name_map = ingest_cfg['flights_column_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dedf3af6-d129-4e69-a407-e1b2828810f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_renamed_df = flights_dropped_df\n",
    "for key, value in column_name_map.items():\n",
    "    flights_renamed_df = flights_renamed_df.withColumnRenamed(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee3bfcc0-8008-4582-a0ab-603865bc5623",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 4 - Add new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2157309-8ed2-4c83-8cc7-bab9d2ef1a17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flights_final_df = flights_renamed_df.withColumn(\n",
    "    \"flight_id\",\n",
    "    concat_ws(\n",
    "        \"_\",\n",
    "        col(\"flight_num\"),\n",
    "        col(\"origin\"),\n",
    "        col(\"destination\"),\n",
    "        col(\"day_of_month\"),\n",
    "        col(\"month\"),\n",
    "        col(\"year\"),\n",
    "        col(\"dep_time\")\n",
    "    )\n",
    ")\n",
    "flights_final_df = ut.add_ingestion_date(flights_final_df)\n",
    "flights_final_df = flights_final_df.withColumn(\"source\", lit(data_source))\n",
    "flights_final_df = flights_final_df.withColumn(\"file_date\", lit(file_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5576067-d99a-43eb-8b12-d3284908515e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 5 - Write to datalake **incrementally** as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.overwrite_partition(flights_final_df, \"dev_air_travel_processed\", \"flights\", \"flight_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59913039-0704-466e-9350-4a74ea88f45e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (flights_final_df.\n",
    "#  write.mode(\"overwrite\").\n",
    "#  partitionBy(\"year\").\n",
    "#  format(\"parquet\").\n",
    "#  saveAsTable(\"dev_air_travel_processed.flights\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030da865-363a-44a0-bf9c-6c8e7e44f913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.ingest_flights_file",
   "notebookOrigID": 2717744799858034,
   "widgets": {
    "data_source": {
     "currentValue": "ASA Dataverse",
     "nuid": "5ca2e7c2-ea40-45f4-92f7-611a4a345067",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "data_source",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "airlines",
   "language": "python",
   "name": "airlines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
